---
layout: post
title: Review of Image Generation Using CLIP-based Contrastive Models
subtitle: Powerful Image Representations Capturing Both Semantics and Style
tags: [AI, Image Generation, CLIP, Deep Learning]
comments: true
author: Seungyeop Cha
mathjax: true
---

## Abstract

Contrastive models like CLIP learn powerful image representations that capture both semantics and style. To use these representations for image generation, this paper proposes two approaches:

1. **Prior** : Generate CLIP image embeddings from text captions.
2. **Decoder** : Generate images conditioned on the image embeddings.

This approach has the following effects:
- **Enhanced photorealism and caption similarity**: Generate images that are both realistic and highly similar to the text caption.
- **Improved image diversity**: Generate a wider variety of images.
- **Preservation of semantics and style**: Generate images that retain essential semantics and style while modifying only non-essential details.

Additionally, the shared embedding space of CLIP enables zero-shot language-based image manipulation. A diffusion model is used as the decoder, significantly outperforming traditional autoregressive models.

**Zero-shot**: refers to the model’s ability to perform tasks or classify new categories without additional training.

## Introduction

CLIP has recently emerged as a robust image representation model with exceptional zero-shot capabilities. Similarly, diffusion models have achieved state-of-the-art results in image and video generation, prioritizing realism over diversity.

One of CLIP’s key strengths lies in its ability to modify images by altering the semantics of their associated text captions, a task that is inefficient in GANs. Additionally, CLIP provides insights into which image features are recognized or ignored during manipulation.

By combining CLIP decoders and priors, diverse image generation becomes possible. Methods to pretrain diffusion models in the latent space further enhance computational efficiency, effectively reversing CLIP’s typical image-to-text application.

![1](/assets/img/Dall2/1.png)

## METHOD

The training set consists of \( (x, y) \), where \( x \) represents an image and \( y \) represents text. From this, the CLIP image and text embeddings \( z_i \) and \( z_t \) are derived. Using these, a text-to-image generation stack is designed with two components:

\\( P(z_i \mid y) \\): A model that generates the CLIP image embedding \( z_i \), conditioned on the caption \( y \).  

\\( P(x \mid z_i, y) \\): A model that generates an image conditioned on the CLIP image embedding \( z_i \) and optionally on the caption \( y \).  

The CLIP decoder enables training up to \( z_i \), allowing the following equation to be computed:  

\\(P(x \mid y) = P(x, z_i \mid y) = P(x \mid z_i, y) P(z_i \mid y)\\)

1. **Decoder**

A diffusion model is used to generate images, conditioned on the prior-generated CLIP image embedding.

![2](/assets/img/Dall2/2.png)
The architecture of the GLIDE model is slightly modified. Specifically, the CLIP embedding is added to the existing timestep embedding. Then, the CLIP embedding is projected into four additional tokens, which are concatenated with the output sequence of the GLIDE text encoder.

The classifier-free guidance technique is employed, which enhances image generation quality by adjusting conditional information without using a classifier. For example, during training, 10% of the CLIP embeddings are set to zero, or 50% of the text captions are removed. This makes the model robust for generating high-quality images.

Two upsampling steps are performed: from 64×64 to 256×256, and then from 256×256 to 1024×1024. Various degradations are applied to improve robustness (Gaussian blur in the first step, and BSR degradation in the second). This approach is referred to as ADMNET.

2. **Prior**

Generating images directly from text captions requires a prior model to map y to \\( z_i \\)

- **Autoregressive prior**

A method of converting CLIP image embeddings into discrete code sequences and sequentially predicting them one by one based on the text caption.

In the process of performing AR efficiently,

Using the SAM technique, the evaluation index is improved by lowering the ranking of the CLIP expression space.
Even if the main components are maintained from 1024 to 319 using pca, most information can be maintained.
Quantize the reduced 319 dimensional values ​​into 1024 discrete buckets.
Trained with Transformer’s causal attention mask

Through the above processes, the number of tokens that need to be predicted can be reduced by 3 times.

- **Diffusion prior** 

Continuous vector \\( z_i \\) is reconstructed through a Gaussian diffusion model given the conditions caption y. Only uses the decoder of the transformer.

the encoded text, the CLIP text embedding, an embedding for the diffusion timestep, the noised CLIP image embedding, and a final embedding
Train in this order

During sampling, two \\( z_i \\) are generated and the sample with the highest dot product with \\( z_t \\) is selected. The loss is as follows.

$$
L_{\text{prior}} = \mathbb{E}_{t \sim [1, T], z_i^{(t)} \sim q_t} \left[ \left\| f_{\theta}(z_i^{(t)}, t, y) - z_i \right\|^2 \right]
$$


## Image Manipulations

This paper's approach allows encoding image x as a bipartite latent representation of ( \\( z_i \\), \\( x^T \\) ), where \\( z_i \\) is recognized by CLIP. As an image side description, encoded with CLIP's image encoder, \\( x^T \\) is the residual information to return to the image, which can be obtained through DDIM Inversion. Contains information to reconstruct

This paper explains three types of image transformation using this bipartite latent representation.

- **Variations**

Through image x, it is possible to create images with the same essential content but different shapes and directions. For this, use DDIM with \\( \eta > 0 \\). If \\( \eta = 0 \\), it is decided. This is because, as the value of η increases, randomness increases in terms of shape and direction.

![3](/assets/img/Dall2/3.png)

- **Interpolations**

It is also possible to mix \\( x_1 \\) and \\( x_2 \\) The image is mixed by going back and forth between the two embeddings \\( z_{i1} \\) and \\( z_{i2} \\). For this, spherical interpolation is used to obtain \\( z_θ = slerp(z_{i1} ,z_{i2} , θ)) \\) Here theta is the angle of rotation between \\( z_{i1} \\) \\( z_{i2} \\)

- **Text Diffs**

The biggest advantage of using CLIP is that it embeds img and text in the same latent space. This makes language-guided image manipulations (i.e., text diffs) possible, which is a technology that modifies images when new text is given. Existing (\\( x_0 \\), \\( x_{i0} \\)) When \\( z_{i1} \\) enters \\( z_d = norm(z_t − z_{t0}) \\) and modify it by calculating slerp \\( z_i,z_d,θ \\)
